<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Background of Reinforcement Learning | WorldHellooo's Blog</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Background of Reinforcement Learning</h1><a id="logo" href="/.">WorldHellooo's Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Background of Reinforcement Learning</h1><div class="post-meta">Apr 17, 2017<span> | </span><span class="category"><a href="/categories/Reinforcement-Learing/">Reinforcement Learing</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="post-content"><h2 id="增强学习"><a href="#增强学习" class="headerlink" title="增强学习"></a>增强学习</h2><p>增强学习的思想来自于对自然学习的观察，比如人类获取知识的主要途径就是通过与环境的交互iteractions。增强学习问题研究what to do，即如何从situations映射到actions，从而最大化一个数值形式的reward。本质上看，这是个闭环问题closed-loop，因为actions会影响后来的输入，即situations。最有趣，也是最有挑战性的一点是，actions不光影响即时的reward，也会影响后续所有的rewards。</p>
<p>增强学习不同于当前机器学习领域研究最多的监督学习。监督学习从标注训练集中学习，每个样本是一个situation和正确的action的集合。比如常见的分类任务中，给定一幅图像（situation），算法需要给出对应的分类（action）。监督学习的目标是实现正确的泛化，即能够正确地应对训练集中未出现过的situation。但是对于交互式的问题，监督学习并不适用。因为在交互问题中，算法无法得到足够而正确的标注样本进行训练。agent必须从自身的经验中学习。</p>
<p>增强学习也不同于无监督学习。无监督学习通常关注于从未标注数据中寻找隐藏的分布结构。</p>
<p>因此，增强学习是与监督学习和无监督学习并列的关系。</p>
<h3 id="exploration-exploitation-dilemma"><a href="#exploration-exploitation-dilemma" class="headerlink" title="exploration-exploitation dilemma"></a>exploration-exploitation dilemma</h3><p>增强学习要面临的挑战是探索（exploration）和开发（exploitation）之间的trade-off。为了获得大量reward，agent必须偏向于选择那些它过去尝试过，并被证明能够高效产生reward的actions。但是与此同时，为了探索这些actions，agent也必须尝试那些之前没有选择过的action。因此，agent必须<strong>exploit</strong>已知的内容以获取最大reward，也必须保证一定的<strong>explore</strong>，以助于将来做出更好的action选择。</p>
<p>###<br>增强学习的另一个特点是，它明确地从全局角度考虑了一个目标导向的agent与未知环境的交互问题。</p>
<p>增强学习基于一个完整的、交互式的、目标导向的agent。agent有明确的目标，能够感知他们所在的环境的某些方面，并且能够选择actions来影响环境。因为增强学习涉及了规划的问题，所以agent必须考虑规划（planning）和实时动作选择（real-time action selection）之间的相互影响，以及如何获取和改善环境模型。</p>
<p>这和许多其他只关注子问题的方法不同。（可能意思是说不是general的）。比如监督学习并没有明确地解释学习到的能力为什么会work。(挖个坑，等理解深点再来补充)</p>
<h2 id="增强学习的组成元素"><a href="#增强学习的组成元素" class="headerlink" title="增强学习的组成元素"></a>增强学习的组成元素</h2><h3 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h3><p>给定一个时间状态，policy决定了agent会做出什么行动。粗略地讲，policy将感知到的环境状态映射到要采纳的动作。某种程度上有点类似生物学中的条件反射。policy可能是简单的映射函数或者数值表，也可能是复杂的搜索过程。policy是一个agent的核心，决定了agent的行为。一般，policy都是有随机性的（stochastic）。（可能是为了保证探索和开发之间的平衡）</p>
<h3 id="reward"><a href="#reward" class="headerlink" title="reward"></a>reward</h3><p>reward信号定义了增强学习的目标。每个时间节点，环境都会给agent一个相应作为reward。而agent唯一的目标就是在长时运行中最大化reward总和。所以说，reward是agent改变policy的动力。</p>
<h3 id="value-function"><a href="#value-function" class="headerlink" title="value function"></a>value function</h3><p>reward信号表示在某个时刻的感知中，什么是有益的；而value function则是从长远的角度考虑。粗略地讲，某个时刻的value，等于该状态的reward，累积后续所有预期状态的reward，得到的reward总和。所以，value function给agent带来了长远的规划能力。</p>
<h3 id="environment-model（optional）"><a href="#environment-model（optional）" class="headerlink" title="environment model（optional）"></a>environment model（optional）</h3><p>env model是对环境的建模，实现如下功能：给定一个state和action，model会反馈出下一个state和reward。</p>
<h2 id="样例-Tic-Tac-Toe"><a href="#样例-Tic-Tac-Toe" class="headerlink" title="样例 Tic-Tac-Toe"></a>样例 Tic-Tac-Toe</h2><p>Tic-Tac-Toe是一个简单的棋盘类游戏，规则有点类似五子棋。两个棋手对战，轮流放棋子，在一个三乘三的棋盘中，首先完成一行、一列或一条对角线的一方获胜。</p>
<p>看起来很简单的游戏却很难用传统的方法来解决。比如动态规划需要知道对手的完整表述，包括对手在某个状态下采取什么行动的概率。这在实际中是不可能得到的。</p>
<p>进化方法（evolutionary method）直接搜索policy space，寻找一个胜率最高的policy。例如，遗传算法（genetic algorithm）会维持并进化一个population，在多次迭代中搜索最优的policy。</p>
<p>基于value function的方法则采用另外一种思路。首先，建立一个数表，代表所有可能的states和它们对应的胜利概率估计。比如，某个状态中，有一行全部被agent的棋子占据，则该状态对应的value就是1；如果全部被对手的棋子占据，则value为0。除了这两种极端情况，其他状态的初始value被设定为0.5。这个表就是value function的具体形式。在游戏中，我们观察一次行动（即放置一颗棋子到棋盘上任意空位处）可能会产生的states（不同的棋子摆放格局）和它们对应的表中的value。大多数情况下，我们会依据贪婪的原则选择value最大的那个state对应的行动，称作greedy move；但是有的时候也会随机地选择行动，称作exploratory move。</p>
<p>在greedy move之后，需要更新move之前的状态的value。设s表示move前状态，s’表示move之后的状态，V(:)表示value function，$\alpha$为学习率：<br>$$V(s) = V(s) + \alpha[V(s’) - V(s)]$$</p>
<p>为了评估一个policy，进化方法会先固定该policy参数，然后多次模拟对战，取胜利的频率作为该policy胜率的无偏估计。但是每次更新policy都需要在很多次对战之后，并且只利用了每次对战的结果信息，忽视了对战中发生了什么。</p>
<p>相反的，增强学习评估对战中的每个状态。这两种方法都实现了对policy space的搜索，但是后者利用了更多信息，也更加合理。</p>
<p>从本例中，还能看到增强学习的几个关键特征。首先是，从与环境的交互（对战）中学习。其次，有一个清晰的目标，考虑、预见了每次行动对未来的影响。所以，本例中的一个简单的增强学习agent就可能能够通过多步move来给对手设置陷阱。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://worldhellooo.github.io/2017/04/17/Background of Reinforcement Learning/" data-id="cj31g7a0d0000u4qmdn8arngu" class="article-share-link">分享到</a><div class="tags"><a href="/tags/增强学习/">增强学习</a></div><div class="post-nav"><a href="/2017/04/17/翻译 Region of interest pooling explained/" class="pre">翻译 Region of interest pooling explained</a><a href="/2017/03/22/SiamFC改进思路/" class="next">SiamFC算法改进思路</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://worldhellooo.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Computer-Vision/">Computer Vision</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learing/">Reinforcement Learing</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Visual-Tracking/">Visual Tracking</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/实例搜索-孪生结果-目标追踪/" style="font-size: 15px;">实例搜索 孪生结果 目标追踪</a> <a href="/tags/目标追踪-研究背景-主流思路/" style="font-size: 15px;">目标追踪 研究背景 主流思路</a> <a href="/tags/RoIPooling-翻译/" style="font-size: 15px;">RoIPooling 翻译</a> <a href="/tags/目标追踪/" style="font-size: 15px;">目标追踪</a> <a href="/tags/增强学习/" style="font-size: 15px;">增强学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/04/17/翻译 Region of interest pooling explained/">翻译 Region of interest pooling explained</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/17/Background of Reinforcement Learning/">Background of Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/22/SiamFC改进思路/">SiamFC算法改进思路</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/21/深度网络训练中的一些参数/">深度网络训练中的一些技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/20/SINT/">Intro of Siamese Instance Search for Tracking</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/19/目标追踪经典算法系列/">目标追踪算法之研究背景</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/19/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://blog.jcix.top" title="Zhangjc's blog" target="_blank">Zhangjc's blog</a><ul></ul><a href="http://chaway.github.io" title="Chaway's blog" target="_blank">Chaway's blog</a><ul></ul><a href="http://www.votchallenge.net" title="VOT Challenge" target="_blank">VOT Challenge</a><ul></ul><a href="http://www.visual-tracking.net" title="OTB Benchmark" target="_blank">OTB Benchmark</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">WorldHellooo's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>