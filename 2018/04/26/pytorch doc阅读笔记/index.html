<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>pytorch doc阅读笔记 | WorldHellooo's Blog</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">pytorch doc阅读笔记</h1><a id="logo" href="/.">WorldHellooo's Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">pytorch doc阅读笔记</h1><div class="post-meta">Apr 26, 2018<span> | </span><span class="category"><a href="/categories/笔记/">笔记</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="post-content"><p>[TOC]</p>
<h2 id="torch特性"><a href="#torch特性" class="headerlink" title="torch特性"></a>torch特性</h2><p>待补充</p>
<h2 id="torch-Tensor"><a href="#torch-Tensor" class="headerlink" title="torch.Tensor"></a>torch.Tensor</h2><h3 id="expand方法"><a href="#expand方法" class="headerlink" title="expand方法"></a>expand方法</h3><p>expand方法并不会开辟新的内存，只是在已有tensor上创建了新的view，修改原tensor，会影响到expand返回的变量。<br>举例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">a.unsqueeze_(<span class="number">1</span>)</div><div class="line">b = a.expand(<span class="number">-1</span>,<span class="number">4</span>)</div><div class="line">b = <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></div><div class="line">    <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></div><div class="line">    <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span></div><div class="line">a[<span class="number">1</span>] = <span class="number">4</span></div><div class="line">b = <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></div><div class="line">    <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span></div><div class="line">    <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span></div></pre></td></tr></table></figure></p>
<h3 id="repeat方法"><a href="#repeat方法" class="headerlink" title="repeat方法"></a>repeat方法</h3><p>和expand不同，该方法拷贝了tensor的数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">x = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</div><div class="line">x.repeat(<span class="number">3</span>,<span class="number">2</span>)</div><div class="line">x = <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span></div><div class="line">    <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span></div><div class="line">    <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span></div></pre></td></tr></table></figure></p>
<h3 id="gather"><a href="#gather" class="headerlink" title="gather"></a>gather</h3><p>同torch.gather()方法。其返回变量开辟了新的内存。<br>gather方法的输入为一个input tensor，一个操作轴dim，一个LongTensor类型的index。<br>首先index的维度要与input tensor相匹配，比如input的size为3×4，有2个维度，index也要通过view或者unsqueeze等方法调整到2个维度，且index要与返回的tensor大小相同。<br>其次，dim=0时，表示的是根据index从input中逐列挑选，以构成并返回一个与input行维度相同的行tensor；dim=1也以此类比。</p>
<h3 id="resize"><a href="#resize" class="headerlink" title="resize_"></a>resize_</h3><p>将tensor大小调整为指定大小。当指定size大于当前tensor的size时，将底层存储调整到与指定size相同。如果指定size小于当前tensor的size时，保持底层存储不变，调整tensor的size。（也就是说并不会丢失数据）</p>
<h3 id="scatter"><a href="#scatter" class="headerlink" title="scatter_"></a>scatter_</h3><p>将一个tensor x按照index确定的索引写入本tensor中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = torch.rand(<span class="number">2</span>,<span class="number">5</span>)</div><div class="line">torch.zeros(<span class="number">3</span>,<span class="number">5</span>).scatter_(<span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>], [<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]]),x)</div></pre></td></tr></table></figure></p>
<p>index的size必须要跟x的size相对应。<br>第一个参数dim=0表示沿着zeros矩阵的第一个维度进行scatter，例如上例子中index矩阵的第一行[0,1,2,0,0]分别代表x的第一行中各项在新矩阵的第一个维度中的下标。</p>
<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><h3 id="torch-nn-Paramter"><a href="#torch-nn-Paramter" class="headerlink" title="torch.nn.Paramter"></a>torch.nn.Paramter</h3><p>Parameters类是Variable的子类，但是它有一个非常独特的属性：当它被分配为Module的属性时，它会自动加到Module的参数列表中（即出现在parameters()迭代器中）。当我们需要在模型中缓存一些变量时（例如RNN的最后一层hidden state输出），我们就可以使用Variable而非Parameters，这样就可以避免将这些缓存作为模型中的参数变量。<br>除此之外，parameters不能够被设置为volatile，并且默认requires_grad=True。而Variable默认requires_grad=False。</p>
<h3 id="torch-nn-Module"><a href="#torch-nn-Module" class="headerlink" title="torch.nn.Module"></a>torch.nn.Module</h3><p>Module是所有神经网络类的基类。构建一个新的神经网络模型，只需要定义一个继承Module的新类，并在<strong>init</strong>方法中定义网络结构，再重载一个forward()函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Model, self).__init__()</div><div class="line">        self.fc1 = nn.Linear(<span class="number">2</span>,<span class="number">2</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">2</span>,<span class="number">2</span>)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        <span class="keyword">return</span> x</div></pre></td></tr></table></figure></p>
<h4 id="torch-nn-Module-register-parameter-name-param"><a href="#torch-nn-Module-register-parameter-name-param" class="headerlink" title="torch.nn.Module.register_parameter(name, param)"></a>torch.nn.Module.register_parameter(name, param)</h4><p>利用上面那种形式定义的网络，会自动把所有子模块的参数加入到模型的_parameters字典中。该方法则实现手动向module中加入一个parameter。</p>
<h4 id="torch-nn-Module-register-buffer-name-tensor"><a href="#torch-nn-Module-register-buffer-name-tensor" class="headerlink" title="torch.nn.Module.register_buffer(name, tensor)"></a>torch.nn.Module.register_buffer(name, tensor)</h4><p>这个方法通常用来注册创建一个长期有效但不应属于模型参数的变量。如果变量名没被占用，则将tensor放入_buffers字典中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.register_buffer(<span class="string">"running_mean"</span>, torch.zeros(num_features))</div></pre></td></tr></table></figure></p>
<h4 id="torch-nn-Module-add-module-name-module"><a href="#torch-nn-Module-add-module-name-module" class="headerlink" title="torch.nn.Module.add_module(name, module)"></a>torch.nn.Module.add_module(name, module)</h4><p>将一个子模块加入到当前模型。如果名字未被占用，则将该子模块加入_modules字典中。</p>
<h4 id="torch-nn-Module-apply-fn"><a href="#torch-nn-Module-apply-fn" class="headerlink" title="torch.nn.Module.apply(fn)"></a>torch.nn.Module.apply(fn)</h4><p>对模型中所有的子模块应用fn方法。通常用来进行初始化。</p>
<h4 id="hook-in-Module"><a href="#hook-in-Module" class="headerlink" title="hook in Module"></a>hook in Module</h4><p>为一个module注册一个hook函数，在计算梯度时自动触发。在不用修改主体代码的前提下，能够实现一些额外的功能，相当于是把它挂在了主体代码上，所以称为“钩子”。在pytorch中，会默认释放掉中间变量以减少对内存的压力。因此在训练一个网络时，想要提取中间层的参数、特征图的时候，就必须要利用到hook。<br>先贴一下Module的<strong>call</strong>方法实现的源码，后面会解释。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *input, **kwargs)</span>:</span></div><div class="line">    <span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_pre_hooks.values():</div><div class="line">        hook(self, input)</div><div class="line">    <span class="keyword">if</span> torch.jit._tracing:</div><div class="line">        result = self._slow_forward(*input, **kwargs)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        result = self.forward(*input, **kwargs)</div><div class="line">    <span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_hooks.values():</div><div class="line">        hook_result = hook(self, input, result)</div><div class="line">        <span class="keyword">if</span> hook_result <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">raise</span> RuntimeError(</div><div class="line">                <span class="string">"forward hooks should never return any values, but '&#123;&#125;'"</span></div><div class="line">                <span class="string">"didn't return None"</span>.format(hook))</div><div class="line">    <span class="keyword">if</span> len(self._backward_hooks) &gt; <span class="number">0</span>:</div><div class="line">        var = result</div><div class="line">        <span class="keyword">while</span> <span class="keyword">not</span> isinstance(var, Variable):</div><div class="line">            <span class="keyword">if</span> isinstance(var, dict):</div><div class="line">                var = next((v <span class="keyword">for</span> v <span class="keyword">in</span> var.values() <span class="keyword">if</span> isinstance(v, Variable)))</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                var = var[<span class="number">0</span>]</div><div class="line">        grad_fn = var.grad_fn</div><div class="line">        <span class="keyword">if</span> grad_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">for</span> hook <span class="keyword">in</span> self._backward_hooks.values():</div><div class="line">                wrapper = functools.partial(hook, self)</div><div class="line">                functools.update_wrapper(wrapper, hook)</div><div class="line">                grad_fn.register_hook(wrapper)</div><div class="line">    <span class="keyword">return</span> result</div></pre></td></tr></table></figure></p>
<h5 id="torch-nn-Module-register-forward-pre-hook-hook"><a href="#torch-nn-Module-register-forward-pre-hook-hook" class="headerlink" title="torch.nn.Module.register_forward_pre_hook(hook)"></a>torch.nn.Module.register_forward_pre_hook(hook)</h5><p>由此方法注册的hook会加入到_forward_pre_hooks字典中。由Module的源码中<strong>call</strong>函数可以看到，此处定义的hook函数会在forward之前执行，且没有返回值。</p>
<h5 id="torch-nn-Module-register-forward-hook-hook"><a href="#torch-nn-Module-register-forward-hook-hook" class="headerlink" title="torch.nn.Module.register_forward_hook(hook)"></a>torch.nn.Module.register_forward_hook(hook)</h5><p>由此方法注册的hook会加入到_forwardhooks字典中，在forward计算后执行。此处的hook函数返回值必须为None，不能修改输入项。因此通常只是用来作为中间结果的查看器。</p>
<h5 id="torch-nn-Module-register-backward-hook-hook"><a href="#torch-nn-Module-register-backward-hook-hook" class="headerlink" title="torch.nn.Module.register_backward_hook(hook)"></a>torch.nn.Module.register_backward_hook(hook)</h5><p>由此方法注册的hook会加入_backward_hooks中。执行时通过注册为Variable的hook函数来实现。这里的hook可以通过返回新的梯度来取代之前的梯度。<br>此处的hook函数应该定义为如下形式：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hook(module, grad_input，grad_output) -&gt; Variable or None</div></pre></td></tr></table></figure></p>
<h4 id="torch-nn-Module-state-dict"><a href="#torch-nn-Module-state-dict" class="headerlink" title="torch.nn.Module.state_dict"></a>torch.nn.Module.state_dict</h4><p>返回一个包含_parameters、_buffers、各个modules的statedict的OrderedDict()</p>
<h4 id="load-state-dict"><a href="#load-state-dict" class="headerlink" title="load_state_dict"></a>load_state_dict</h4><p>读入一个OrderDict对象</p>
<h3 id="torch-nn-Sequential"><a href="#torch-nn-Sequential" class="headerlink" title="torch.nn.Sequential"></a>torch.nn.Sequential</h3><p>一个序列容器，其中的modules将按照加入的顺序执行。该容器有两种初始化方式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">model = nn.Sequential(</div><div class="line">            nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</div><div class="line">            nn.ReLU(),</div><div class="line">            nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</div><div class="line">            nn.ReLU()</div><div class="line">    )</div><div class="line"><span class="comment"># or using OrderedDict</span></div><div class="line">model = nn.Sequential(</div><div class="line">    OrderedDict([</div><div class="line">        (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</div><div class="line">        (<span class="string">'relu1'</span>, nn.ReLU()),</div><div class="line">        (<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</div><div class="line">        (<span class="string">'relu2'</span>, nn.ReLU())</div><div class="line">    ])</div><div class="line">    )</div></pre></td></tr></table></figure></p>
<h3 id="torch-nn-ModuleList"><a href="#torch-nn-ModuleList" class="headerlink" title="torch.nn.ModuleList"></a>torch.nn.ModuleList</h3><p>ModuleList将子module保存在一个列表里，可以像普通的list对象一样，通过下标的方式索引。同样地，ModuleList类也具有类似于list的append、extend方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Model, self).__init__()</div><div class="line">        self.linears = nn.ModuleList([nn.Linear(<span class="number">10</span>,<span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)])</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></div><div class="line">        <span class="keyword">for</span> i,l <span class="keyword">in</span> enumerate(self.linears):</div><div class="line">            x = self.linear[i // <span class="number">2</span>](x) + l(x)</div><div class="line">        <span class="keyword">return</span> x</div></pre></td></tr></table></figure></p>
<h3 id="torch-nn-Conv1d"><a href="#torch-nn-Conv1d" class="headerlink" title="torch.nn.Conv1d"></a>torch.nn.Conv1d</h3><p>对于一个输入信号进行一维卷积操作。主要参数如下：</p>
<ul>
<li><strong>in_channels:</strong>输入通道数</li>
<li><strong>out_channels:</strong>输出通道数</li>
<li><strong>kernel_size:</strong>核大小</li>
<li><strong>stride=1:</strong>步长</li>
<li><strong>padding=0:</strong>填充（由于卷积核和输入信号的大小不一定匹配，因此可能会损失一些边界信息，所以可以采用padding进行填充）</li>
<li><strong>dilation=1:</strong>控制卷积核点之间的间距<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank" rel="external">可视化</a></li>
<li><strong>groups=1:</strong>如果groups不为1，则将输入的通道分为groups组，对应专门的卷积核。当<strong><em>groups=in_channels，out_channels=K</em>in_channels*</strong>时，就是所谓的depthwise convolution，即对于每个通道单独做卷积处理。</li>
<li><strong>bias=True:</strong> 偏差项<br>输入、输出信号的形式：<br><strong>Input:</strong> $(N, C_{in}, L_{in})$<br><strong>Output:</strong> $(N, C_{out}, L_{out})$，这里<br>$$L_{out} = floor((L_{in}+2*padding-dilation*(kernel\_size-1)-1)/stride+1)$$<br>计算公式为：<br>$$out(N_i, C_{out_j})=bias(C_{out_j})+\sum_{k=0}^{C_{in}-1}weight(C_{out_j},k)*input(N_i, k)$$</li>
</ul>
<h3 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d"></a>torch.nn.Conv2d</h3><p>参数基本与Conv1d相同。但是由于输入输出都是二维信号，所以在kernelsize、stride、dilation等变量上可以采用不对等的形式。如下面代码所示。<br>输入、输出信号的形式：<br><strong>Input:</strong> $(N, C_{in}, H_{in}, W_{in})$<br><strong>Output:</strong> $(N, C_{out}, H_{out}, W_{out})$<br>$$H_{out} = floor((H_{in}+2*padding[0]-dilation[0]*(kernel\<em>size[0]-1)-1)/stride[0]+1)$$<br>$$W</em>{out} = floor((W_{in}+2*padding[1]-dilation[1]*(kernel\_size[1]-1)-1)/stride[1]+1)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## square kernels and equal stride</span></div><div class="line">m = nn.Conv2d(in_channels=<span class="number">16</span>, out_channels=<span class="number">33</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</div><div class="line"><span class="comment">## non-square kernels and unequal stride and with padding and dilation</span></div><div class="line">m = nn.Conv2d(in_channels=<span class="number">16</span>, out_channels=<span class="number">33</span>, kernel_size=(<span class="number">3</span>,<span class="number">5</span>), stride=(<span class="number">2</span>,<span class="number">1</span>), padding=(<span class="number">4</span>,<span class="number">2</span>), dilation=(<span class="number">3</span>,<span class="number">1</span>))</div></pre></td></tr></table></figure>
<h3 id="torch-nn-ConvTranspose2d"><a href="#torch-nn-ConvTranspose2d" class="headerlink" title="torch.nn.ConvTranspose2d"></a>torch.nn.ConvTranspose2d</h3><p>这个module可以视作Conv2d关于input的梯度。它可以被看做解卷积（Deconvolution）操作，但并不是真正的解卷积操作。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">input = Variable(torch.randn(<span class="number">1</span>,<span class="number">16</span>,<span class="number">12</span>,<span class="number">12</span>))</div><div class="line">downsample = nn.Conv2d(<span class="number">16</span>,<span class="number">16</span>,<span class="number">3</span>,stride=<span class="number">2</span>, padding=<span class="number">1</span>)</div><div class="line">upsample = nn.ConvTranspose2d(<span class="number">16</span>,<span class="number">16</span>,<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>)</div><div class="line">h = downsample(input)</div><div class="line">output = upsample(h, output_size=input.size())</div><div class="line"><span class="comment">## 计算重构损失</span></div></pre></td></tr></table></figure></p>
<h3 id="torch-nn-MaxPool2d"><a href="#torch-nn-MaxPool2d" class="headerlink" title="torch.nn.MaxPool2d"></a>torch.nn.MaxPool2d</h3><p>主要参数</p>
<ul>
<li><strong>kernel_size</strong></li>
<li><strong>stride=1</strong></li>
<li><strong>padding=0</strong></li>
<li><strong>dilation=1</strong></li>
<li><strong>return_indices:</strong>如果设为True，则返回outputs对应的最大点的坐标（unpooling操作能用到）</li>
<li><strong>ceil_mode:</strong>如果设置为True，使用<em>ceil</em>代替<em>floor</em>来计算输出的shape</li>
</ul>
<h3 id="torch-nn-MaxUnpool2d"><a href="#torch-nn-MaxUnpool2d" class="headerlink" title="torch.nn.MaxUnpool2d"></a>torch.nn.MaxUnpool2d</h3><p>MaxPooling是无法完全可逆的，因为损失了一些非最大值的信息。<br>MaxUnpool接收MaxPool返回的最大值坐标，完成这部分信息的恢复，其他的非最大值处都被设置为0。<br>由于MaxPooling可以将多种输入大小映射到同样的输出大小，因此，反推结果可能就不唯一。为了解决这一点，可以在调用时，加入output_size参数指定。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">pool = nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>, return_indices=<span class="keyword">True</span>)</div><div class="line">unpool = nn.MaxUnpool2d(<span class="number">2</span>,stride=<span class="number">2</span>)</div><div class="line">input = Variable(torch.Tensor([[[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</div><div class="line">                                    [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],</div><div class="line">                                    [<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>],</div><div class="line">                                    [<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>]]]]))</div><div class="line">output, indices = pool(intput)</div><div class="line">unpool(output, indices)</div><div class="line">unpool(output, indices, output_size=torch.Size([<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>]))</div></pre></td></tr></table></figure></p>
<h3 id="torch-nn-FractionalMaxPool2d"><a href="#torch-nn-FractionalMaxPool2d" class="headerlink" title="torch.nn.FractionalMaxPool2d"></a>torch.nn.FractionalMaxPool2d</h3><p>不同于传统pooling基于矩形关注区域操作，fractionalmaxpooling引入了更多随机性，它的stride是由目标输出大小确定的随机步长。</p>
<h3 id="torch-nn-LPPool2d"><a href="#torch-nn-LPPool2d" class="headerlink" title="torch.nn.LPPool2d"></a>torch.nn.LPPool2d</h3><p>幂平均池化。<br>$$f(x)=pow(sum(X,p),1/p)$$</p>
<h3 id="torch-nn-AdaptiveMaxPool2d"><a href="#torch-nn-AdaptiveMaxPool2d" class="headerlink" title="torch.nn.AdaptiveMaxPool2d"></a>torch.nn.AdaptiveMaxPool2d</h3><p>自适应最大值池化。即指定输出的output_size，算法自动设置各项参数。如果为None则保持跟输入相同的大小。</p>
<h3 id="torch-nn-Softmax2d"><a href="#torch-nn-Softmax2d" class="headerlink" title="torch.nn.Softmax2d"></a>torch.nn.Softmax2d</h3><p>输入数据$(N,C,H,W)$，输出同大小。对于每个通道求softmax。</p>
<h3 id="torch-nn-BatchNorm2d"><a href="#torch-nn-BatchNorm2d" class="headerlink" title="torch.nn.BatchNorm2d"></a>torch.nn.BatchNorm2d</h3><p>$$y = \frac{x-mean[x]}{\sqrt{Var[x]+\epsilon}}*gamma+beta$$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">m = nn.BatchNorm2d(<span class="number">100</span>)</div><div class="line">m = nn.BatchNorm2d(<span class="number">100</span>, affine=<span class="keyword">False</span>)</div><div class="line">input = Variable(torch.randn(<span class="number">20</span>,<span class="number">100</span>,<span class="number">35</span>,<span class="number">45</span>))</div><div class="line">output = m(input)</div></pre></td></tr></table></figure></p>
<h3 id="torch-nn-RNN"><a href="#torch-nn-RNN" class="headerlink" title="torch.nn.RNN"></a>torch.nn.RNN</h3><p>Multi-layer Elman RNN with tanh or ReLU<br>$$h_t = activation(w_{ih} <em> x_t + b_{ih} + w_{hh} </em> h_{(t-1)} + b_{hh})$$<br>其中activation可以是tanh或relu。这是一个最简单的RNN形式，每一时刻的$h_t$都由当前时刻的输入$w_{ih} <em> x_t + b_{ih}$和上一时刻的状态$w_{hh} </em> h_{(t-1)} + b_{hh}$共同决定。</p>
<ul>
<li><strong>input:</strong><ul>
<li>input(seq_len, batch, input_size)</li>
<li>h_0(num_layers*num_directions, batch, hidden_size):初始hiddenstate，若不设置则默认为0。</li>
</ul>
</li>
<li><strong>output:</strong><ul>
<li>output(seq_len, batch, hidden_size*num_directions)</li>
<li>h_n(num_layers*num_directions, batch, hidden_size)</li>
</ul>
</li>
</ul>
<h3 id="torch-nn-LSTM"><a href="#torch-nn-LSTM" class="headerlink" title="torch.nn.LSTM"></a>torch.nn.LSTM</h3><p>$$i_t = sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{(t-1)}+b_{hi})$$<br>$$f_t = sigmoid(W_{if}x_t+b_{if}+W_{hf}h_{(t-1)}+b_{hf})$$<br>$$g_t = sigmoid(W_{ig}x_t+b_{ig}+W_{hc}h_{(t-1)}+b_{hg})$$<br>$$o_t = sigmoid(W_{io}x_t+b_{io}+W_{ho}h_{(t-1)}+b_{ho})$$<br>$$c_t = f_t*c_{(t-1)}+i_t*g_t$$<br>$$h_t = o_t*tanh(c_t)$$<br>此处的$h_t$是hidden state，$x_t$是前一层网络在$t$时刻的hidden state（或第一层的输入$input_t$），$i_t, f_t, g_t, o_t$分别是input gate、forget gate、cell gate和output gate。</p>
<h3 id="torch-nn-GRU"><a href="#torch-nn-GRU" class="headerlink" title="torch.nn.GRU"></a>torch.nn.GRU</h3><p>$$r_t = sigmoid(W_{ir}x_t+b_{ir}+W_{hr}h_{(t-1)}+b_{hr})$$<br>$$z_t = sigmoid(W_{iz}x_t+b_{iz}+W_{hz}h_{(t-1)}+b_{hz})$$<br>$$n_t = tanh(W_{in}x_t+b_{in}+r_t*(W_{hn}h_{(t-1)}+b_{hn}))$$<br>$$h_t = (1-z_t)*n_t+z_t*h_{(t-1)}$$<br>此处的$h_t$是hidden state，$x_t$是前一层网络在$t$时刻的hidden state（或第一层的输入$input_t$），$r_t, z_t, n_t$分别是reset gate、input gate和new gate。</p>
<h3 id="torch-nn-RNNCell"><a href="#torch-nn-RNNCell" class="headerlink" title="torch.nn.RNNCell"></a>torch.nn.RNNCell</h3><p>Elman RNN的核心部分。也就是说序列的部分需要自己手动完成，这样也增大了灵活性。<br>$$h’= activation(w_{ih} <em> x + b_{ih} + w_{hh} </em> h + b_{hh})$$</p>
<ul>
<li><strong>input:</strong><ul>
<li>input(batch, input_size)</li>
<li>hidden(batch, hidden_size):初始hidden state</li>
</ul>
</li>
<li><strong>output:</strong><ul>
<li>h’(batch, hidden_size)对于batch中所有个体在下个时刻的hidden state<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">rnn = nn.RNNCell(<span class="number">10</span>,<span class="number">20</span>)</div><div class="line">input = Variable(torch.randn(<span class="number">6</span>,<span class="number">3</span>,<span class="number">10</span>))</div><div class="line">hx = Variable(torch.randn(<span class="number">3</span>,<span class="number">20</span>))</div><div class="line">output = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(input.size(<span class="number">0</span>)):</div><div class="line">    hx = rnn(input[i], hx)</div><div class="line">    output.append(hx)</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="torch-nn-LSTMCell"><a href="#torch-nn-LSTMCell" class="headerlink" title="torch.nn.LSTMCell"></a>torch.nn.LSTMCell</h3><ul>
<li><strong>input:</strong><ul>
<li>input(batch, input_size)</li>
<li>h_0(batch, hidden_size):初始hidden state</li>
<li>c_0(batch, hidden_size):初始cell memory</li>
</ul>
</li>
<li><strong>output:</strong><ul>
<li>h_1(batch, hidden_size):next hidden state for each element in the batch</li>
<li>c_1(batch, hidden_size):next cell state for each element in the batch<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">lstm = nn.LSTMCell(<span class="number">10</span>,<span class="number">20</span>)</div><div class="line">input = Variable(torch.randn(<span class="number">6</span>,<span class="number">3</span>,<span class="number">10</span>))</div><div class="line">hx = Variable(torch.randn(<span class="number">3</span>,<span class="number">20</span>))</div><div class="line">cx = Variable(torch.randn(<span class="number">3</span>,<span class="number">20</span>))</div><div class="line">output = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(input.size(<span class="number">0</span>)):</div><div class="line">    hx, cx = lstm(input[i], hx, cx)</div><div class="line">    output.append(hx)</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="torch-nn-Embedding"><a href="#torch-nn-Embedding" class="headerlink" title="torch.nn.Embedding"></a>torch.nn.Embedding</h3><p>一个速查表，存储着固定大小的字典。通常用来存储word embeddings，并使用下标来取回。输入是一个indices的列表，输出是对应的word embeddings。</p>
<ul>
<li><strong>Parameters</strong><ul>
<li>num_embeddings(int) - 输入的维数</li>
<li>embedding_dim(int) - 输出的向量维数</li>
<li>padding_idx(int, optional) - 设定输出为零的下标</li>
<li>max_norm（float, optional) - 设定输出向量的最大范数上界</li>
<li>norm_type(float, optional) - p-norm的p</li>
<li>scale_grad_by_freq(boolean, optional) - 根据字典中的单词频率来scale梯度</li>
<li>sparse(boolean, optional) - 如果为true，对于权重矩阵的偏导梯度将是一个sparse的tensor。<br>目前只有一部分的optimizer支持sparse gradient：SGD、SparseAdam、Adagrad。<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">embedding = nn.Embedding(<span class="number">10</span>,<span class="number">3</span>)</div><div class="line">input = Variable(torch.Longtensor([[<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">7</span>]]))</div><div class="line">embedding(input)</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="torch-nn-CrossEntropyLoss"><a href="#torch-nn-CrossEntropyLoss" class="headerlink" title="torch.nn.CrossEntropyLoss"></a>torch.nn.CrossEntropyLoss</h3><p>这个类将LogSoftMax和NLLLoss结合在一起。经常用于多分类任务中。可选的参数<em>weight</em>能够给每个类分配权重，这在训练集不均衡时非常有用。<br><em>input</em>需要是一个2D的Tensor，(minibatch, C)。<br>loss的形式为：<br>$$loss(x, class) = -log\frac{(exp(x[class])}{\sum_j exp(x[j]))}$$<br>$$loss(x, class) = -x[class] + log(\sum_j exp(x[j]))$$<br>当<em>weight</em>参数被指定时：<br>$$loss(x, class) = weight[class] * (-x[class] + log(\sum_j exp(x[j])))$$</p>
<ul>
<li><strong>Parameters</strong><ul>
<li>weight(Tensor, optional)维度为C的Tensor</li>
<li>size_average(bool, optional)默认为true，即会对minibatch求avg loss；反之求sum loss</li>
<li>ignore_index(int, optional)设置一个target value，在计算梯度时将会忽略该类</li>
<li>reduce(bool, optional)默认为true，将每个minibatch的loss合并到一起（avg or sum）；如果为false则忽略size_average，返回batch中每个element的loss。</li>
</ul>
</li>
</ul>
<h3 id="torch-nn-NLLLoss"><a href="#torch-nn-NLLLoss" class="headerlink" title="torch.nn.NLLLoss"></a>torch.nn.NLLLoss</h3><p>NLLLoss层前加上一个LogSoftmax层就等价于CrossEntropyLoss层。</p>
<h3 id="torch-nn-PixelShuffle"><a href="#torch-nn-PixelShuffle" class="headerlink" title="torch.nn.PixelShuffle"></a>torch.nn.PixelShuffle</h3><p>利用多通道信息实现空间超分辨操作。</p>
<ul>
<li><strong>Input:</strong> (N, C*upscale_factor^2, H, W)</li>
<li><strong>Output:</strong> (N, C, H<em>upscale_factor, W</em>upscale_factor)<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ps = nn.PixelShuffle(<span class="number">3</span>)</div><div class="line">input = Variable(torch.Tensor(<span class="number">1</span>,<span class="number">9</span>,<span class="number">4</span>,<span class="number">4</span>))</div><div class="line">output = ps(input)</div><div class="line">output.size() <span class="comment"># 1,1,12,12</span></div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="torch-nn-DataParallel"><a href="#torch-nn-DataParallel" class="headerlink" title="torch.nn.DataParallel"></a>torch.nn.DataParallel</h3><p>该容器通过将输入数据划分到不同的设备（一般指GPU）上，来实现给定module的并行化应用。在每一次forward计算时，module被复制到每个设备上，并负责处理部分input。反向计算时，每个复制品的梯度被求和到一起。<br>batch size的大小要大于GPU的数量，最好要能够整除GPU的数量，以保证每个GPU处理相同数量的样本。<br>需要注意的是，module中定义的forward和backward hooks和他的submodules将不会被调用，除非hooks在forward()中被初始化。</p>
<h3 id="torch-nn-DistributedDataParallel"><a href="#torch-nn-DistributedDataParallel" class="headerlink" title="torch.nn.DistributedDataParallel"></a>torch.nn.DistributedDataParallel</h3><p>该容器同样将输入数据划分到不同的设备上来实现并行。不同于DataParallel，该容器将module复制到每个machine和每个device，每个复制品处理一部分输入。在反向计算时，来自每个node的梯度被平均化。<br><strong>Warnings</strong>见pytorch doc。</p>
<h3 id="torch-nn-utils-clip-grad-norm"><a href="#torch-nn-utils-clip-grad-norm" class="headerlink" title="torch.nn.utils.clip_grad_norm"></a>torch.nn.utils.clip_grad_norm</h3><p>限制参数的最大范数值。</p>
<ul>
<li><strong>Parameters</strong><ul>
<li>parameters</li>
<li>max_norm</li>
<li>norm_type</li>
</ul>
</li>
<li><strong>Returns</strong><ul>
<li>所有参数的p-norm值</li>
</ul>
</li>
</ul>
<h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><h3 id="如何使用optimizer"><a href="#如何使用optimizer" class="headerlink" title="如何使用optimizer"></a>如何使用optimizer</h3><p>在定义optimizer时，需要指定被优化的参数。除此之外，还可以指定其他的优化参数，如学习率、weight decay等。如果需要将待优化参数放入GPU，则需要在定义optimizer之前。<br><strong>Example</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</div><div class="line">optimizer = optim.Adam([var1, var2], lr=<span class="number">0.001</span>)</div></pre></td></tr></table></figure></p>
<p>Optimizer也支持对于每个优化对象指定单独的优化参数。传递可迭代的dicts，每一个dict都要定义一个单独的参数组，也要包含“params”关键字，除此之外，也可单独给每个dict指定优化参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">optim.SGD([</div><div class="line">        &#123;<span class="string">'params'</span>: model.base.parameters()&#125;,</div><div class="line">        &#123;<span class="string">'params'</span>: model.classifier.parameters(), <span class="string">'lr'</span>:<span class="number">1e-3</span>&#125;</div><div class="line">        ],</div><div class="line">        lr = <span class="number">1e-2</span>,</div><div class="line">        momentum = <span class="number">0.9</span></div><div class="line">        )</div></pre></td></tr></table></figure></p>
<h3 id="torch-optim-Optimizer"><a href="#torch-optim-Optimizer" class="headerlink" title="torch.optim.Optimizer"></a>torch.optim.Optimizer</h3><p>主要方法：<br><strong>add_param_group:</strong><br>往该optimizer中添加一个新的param group。<br>当finetune一个预训练的网络时非常有用，可以在训练过程中把一开始冻结的参数也加入训练。<br><strong>load_state_dict</strong><br>加载state。<br><strong>state_dict</strong><br>将optimizer的状态以字典的形式返回。<br><strong>step</strong><br>执行一次参数更新操作<br><strong>zero_grad</strong><br>清除所有待优化参数的gradient</p>
<h3 id="如何调整Learning-Rate"><a href="#如何调整Learning-Rate" class="headerlink" title="如何调整Learning Rate"></a>如何调整Learning Rate</h3><p>torch.optim.lr_scheduler提供了几种基于epoch数调整学习率的方法。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://worldhellooo.github.io/2018/04/26/pytorch doc阅读笔记/" data-id="cjgherv2e000g0csef38l37hx" class="article-share-link">分享到</a><div class="tags"></div><div class="post-nav"><a href="/2018/05/18/deepmind论文解读/" class="pre">DeepMind论文：连接多巴胺与元强化学习的新方法</a><a href="/2018/04/18/大数据竞赛经验谈/" class="next">大数据竞赛经验谈</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://worldhellooo.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Computer-Vision/">Computer Vision</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GUI/">GUI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learing/">Reinforcement Learing</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Visual-Tracking/">Visual Tracking</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/competition/">competition</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/笔记/">笔记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/增强学习/" style="font-size: 15px;">增强学习</a> <a href="/tags/实例搜索-孪生结果-目标追踪/" style="font-size: 15px;">实例搜索 孪生结果 目标追踪</a> <a href="/tags/目标追踪/" style="font-size: 15px;">目标追踪</a> <a href="/tags/增强学习-目标追踪/" style="font-size: 15px;">增强学习 目标追踪</a> <a href="/tags/目标追踪-研究背景-主流思路/" style="font-size: 15px;">目标追踪 研究背景 主流思路</a> <a href="/tags/RoIPooling-翻译/" style="font-size: 15px;">RoIPooling 翻译</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/12/12/关于概率和熵的一些概念/">关于概率和熵的一些概念</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/18/deepmind论文解读/">DeepMind论文：连接多巴胺与元强化学习的新方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/26/pytorch doc阅读笔记/">pytorch doc阅读笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/18/大数据竞赛经验谈/">大数据竞赛经验谈</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/12/基于PyQt的多源图像文字描述生成软件界面/">基于PyQt的多源图像文字描述生成软件界面</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/翻译 Region of interest pooling explained/">翻译 Region of interest pooling explained</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/目标追踪经典算法系列/">目标追踪算法之研究背景</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/深度网络训练中的一些参数/">深度网络训练中的一些技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/SINT/">Introduction of Siamese Instance Search for Tracking</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/SiamFC改进思路/">SiamFC算法改进思路</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://blog.jcix.top" title="Zhangjc's blog" target="_blank">Zhangjc's blog</a><ul></ul><a href="http://chaway.github.io" title="Chaway's blog" target="_blank">Chaway's blog</a><ul></ul><a href="http://www.votchallenge.net" title="VOT Challenge" target="_blank">VOT Challenge</a><ul></ul><a href="http://www.visual-tracking.net" title="OTB Benchmark" target="_blank">OTB Benchmark</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">WorldHellooo's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>