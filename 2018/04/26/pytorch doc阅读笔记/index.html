<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>pytorch doc阅读笔记 | WorldHellooo's Blog</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">pytorch doc阅读笔记</h1><a id="logo" href="/.">WorldHellooo's Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">pytorch doc阅读笔记</h1><div class="post-meta">Apr 26, 2018<span> | </span><span class="category"><a href="/categories/笔记/">笔记</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="post-content"><p>[TOC]</p>
<h2 id="torch特性"><a href="#torch特性" class="headerlink" title="torch特性"></a>torch特性</h2><p>待补充</p>
<h2 id="torch-Tensor"><a href="#torch-Tensor" class="headerlink" title="torch.Tensor"></a>torch.Tensor</h2><h3 id="expand方法"><a href="#expand方法" class="headerlink" title="expand方法"></a>expand方法</h3><p>expand方法并不会开辟新的内存，只是在已有tensor上创建了新的view，修改原tensor，会影响到expand返回的变量。<br>举例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">a.unsqueeze_(<span class="number">1</span>)</div><div class="line">b = a.expand(<span class="number">-1</span>,<span class="number">4</span>)</div><div class="line">b = <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></div><div class="line">    <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></div><div class="line">    <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span></div><div class="line">a[<span class="number">1</span>] = <span class="number">4</span></div><div class="line">b = <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></div><div class="line">    <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span></div><div class="line">    <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span></div></pre></td></tr></table></figure></p>
<h3 id="repeat方法"><a href="#repeat方法" class="headerlink" title="repeat方法"></a>repeat方法</h3><p>和expand不同，该方法拷贝了tensor的数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">x = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</div><div class="line">x.repeat(<span class="number">3</span>,<span class="number">2</span>)</div><div class="line">x = <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span></div><div class="line">    <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span></div><div class="line">    <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span></div></pre></td></tr></table></figure></p>
<h3 id="gather"><a href="#gather" class="headerlink" title="gather"></a>gather</h3><p>同torch.gather()方法。其返回变量开辟了新的内存。<br>gather方法的输入为一个input tensor，一个操作轴dim，一个LongTensor类型的index。<br>首先index的维度要与input tensor相匹配，比如input的size为3×4，有2个维度，index也要通过view或者unsqueeze等方法调整到2个维度，且index要与返回的tensor大小相同。<br>其次，dim=0时，表示的是根据index从input中逐列挑选，以构成并返回一个与input行维度相同的行tensor；dim=1也以此类比。</p>
<h3 id="resize"><a href="#resize" class="headerlink" title="resize_"></a>resize_</h3><p>将tensor大小调整为指定大小。当指定size大于当前tensor的size时，将底层存储调整到与指定size相同。如果指定size小于当前tensor的size时，保持底层存储不变，调整tensor的size。（也就是说并不会丢失数据）</p>
<h3 id="scatter"><a href="#scatter" class="headerlink" title="scatter_"></a>scatter_</h3><p>将一个tensor x按照index确定的索引写入本tensor中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = torch.rand(<span class="number">2</span>,<span class="number">5</span>)</div><div class="line">torch.zeros(<span class="number">3</span>,<span class="number">5</span>).scatter_(<span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>], [<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]]),x)</div></pre></td></tr></table></figure></p>
<p>index的size必须要跟x的size相对应。</p>
<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><h3 id="torch-nn-Paramter"><a href="#torch-nn-Paramter" class="headerlink" title="torch.nn.Paramter"></a>torch.nn.Paramter</h3><p>Parameters类是Variable的子类，但是它有一个非常独特的属性：当它被分配为Module的属性时，它会自动加到Module的参数列表中（即出现在parameters()迭代器中）。当我们需要在模型中缓存一些变量时（例如RNN的最后一层hidden state输出），我们就可以使用Variable而非Parameters，这样就可以避免将这些缓存作为模型中的参数变量。<br>除此之外，parameters不能够被设置为volatile，并且默认requires_grad=True。而Variable默认requires_grad=False。</p>
<h3 id="torch-nn-Module"><a href="#torch-nn-Module" class="headerlink" title="torch.nn.Module"></a>torch.nn.Module</h3><p>Module是所有神经网络类的基类。构建一个新的神经网络模型，只需要定义一个继承Module的新类，并在<strong>init</strong>方法中定义网络结构，再重载一个forward()函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Model, self).__init__()</div><div class="line">        self.fc1 = nn.Linear(<span class="number">2</span>,<span class="number">2</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">2</span>,<span class="number">2</span>)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        <span class="keyword">return</span> x</div></pre></td></tr></table></figure></p>
<h4 id="torch-nn-Module-register-parameter-name-param"><a href="#torch-nn-Module-register-parameter-name-param" class="headerlink" title="torch.nn.Module.register_parameter(name, param)"></a>torch.nn.Module.register_parameter(name, param)</h4><p>利用上面那种形式定义的网络，会自动把所有子模块的参数加入到模型的_parameters字典中。该方法则实现手动向module中加入一个parameter。</p>
<h4 id="torch-nn-Module-register-buffer-name-tensor"><a href="#torch-nn-Module-register-buffer-name-tensor" class="headerlink" title="torch.nn.Module.register_buffer(name, tensor)"></a>torch.nn.Module.register_buffer(name, tensor)</h4><p>这个方法通常用来注册创建一个长期有效但不应属于模型参数的变量。如果变量名没被占用，则将tensor放入_buffers字典中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.register_buffer(<span class="string">"running_mean"</span>, torch.zeros(num_features))</div></pre></td></tr></table></figure></p>
<h4 id="torch-nn-Module-add-module-name-module"><a href="#torch-nn-Module-add-module-name-module" class="headerlink" title="torch.nn.Module.add_module(name, module)"></a>torch.nn.Module.add_module(name, module)</h4><p>将一个子模块加入到当前模型。如果名字未被占用，则将该子模块加入_modules字典中。</p>
<h4 id="torch-nn-Module-apply-fn"><a href="#torch-nn-Module-apply-fn" class="headerlink" title="torch.nn.Module.apply(fn)"></a>torch.nn.Module.apply(fn)</h4><p>对模型中所有的子模块应用fn方法。通常用来进行初始化。</p>
<h4 id="hook-in-Module"><a href="#hook-in-Module" class="headerlink" title="hook in Module"></a>hook in Module</h4><p>为一个module注册一个hook函数，在计算梯度时自动触发。在不用修改主体代码的前提下，能够实现一些额外的功能，相当于是把它挂在了主体代码上，所以称为“钩子”。在pytorch中，会默认释放掉中间变量以减少对内存的压力。因此在训练一个网络时，想要提取中间层的参数、特征图的时候，就必须要利用到hook。<br>先贴一下Module的<strong>call</strong>方法实现的源码，后面会解释。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *input, **kwargs)</span>:</span></div><div class="line">    <span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_pre_hooks.values():</div><div class="line">        hook(self, input)</div><div class="line">    <span class="keyword">if</span> torch.jit._tracing:</div><div class="line">        result = self._slow_forward(*input, **kwargs)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        result = self.forward(*input, **kwargs)</div><div class="line">    <span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_hooks.values():</div><div class="line">        hook_result = hook(self, input, result)</div><div class="line">        <span class="keyword">if</span> hook_result <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">raise</span> RuntimeError(</div><div class="line">                <span class="string">"forward hooks should never return any values, but '&#123;&#125;'"</span></div><div class="line">                <span class="string">"didn't return None"</span>.format(hook))</div><div class="line">    <span class="keyword">if</span> len(self._backward_hooks) &gt; <span class="number">0</span>:</div><div class="line">        var = result</div><div class="line">        <span class="keyword">while</span> <span class="keyword">not</span> isinstance(var, Variable):</div><div class="line">            <span class="keyword">if</span> isinstance(var, dict):</div><div class="line">                var = next((v <span class="keyword">for</span> v <span class="keyword">in</span> var.values() <span class="keyword">if</span> isinstance(v, Variable)))</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                var = var[<span class="number">0</span>]</div><div class="line">        grad_fn = var.grad_fn</div><div class="line">        <span class="keyword">if</span> grad_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">for</span> hook <span class="keyword">in</span> self._backward_hooks.values():</div><div class="line">                wrapper = functools.partial(hook, self)</div><div class="line">                functools.update_wrapper(wrapper, hook)</div><div class="line">                grad_fn.register_hook(wrapper)</div><div class="line">    <span class="keyword">return</span> result</div></pre></td></tr></table></figure></p>
<h5 id="torch-nn-Module-register-forward-pre-hook-hook"><a href="#torch-nn-Module-register-forward-pre-hook-hook" class="headerlink" title="torch.nn.Module.register_forward_pre_hook(hook)"></a>torch.nn.Module.register_forward_pre_hook(hook)</h5><p>由此方法注册的hook会加入到_forward_pre_hooks字典中。由Module的源码中<strong>call</strong>函数可以看到，此处定义的hook函数会在forward之前执行，且没有返回值。</p>
<h5 id="torch-nn-Module-register-forward-hook-hook"><a href="#torch-nn-Module-register-forward-hook-hook" class="headerlink" title="torch.nn.Module.register_forward_hook(hook)"></a>torch.nn.Module.register_forward_hook(hook)</h5><p>由此方法注册的hook会加入到_forwardhooks字典中，在forward计算后执行。此处的hook函数返回值必须为None，不能修改输入项。因此通常只是用来作为中间结果的查看器。</p>
<h5 id="torch-nn-Module-register-backward-hook-hook"><a href="#torch-nn-Module-register-backward-hook-hook" class="headerlink" title="torch.nn.Module.register_backward_hook(hook)"></a>torch.nn.Module.register_backward_hook(hook)</h5><p>由此方法注册的hook会加入_backward_hooks中。执行时通过注册为Variable的hook函数来实现。这里的hook可以通过返回新的梯度来取代之前的梯度。<br>此处的hook函数应该定义为如下形式：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hook(module, grad_input，grad_output) -&gt; Variable or None</div></pre></td></tr></table></figure></p>
<h4 id="torch-nn-Module-state-dict"><a href="#torch-nn-Module-state-dict" class="headerlink" title="torch.nn.Module.state_dict"></a>torch.nn.Module.state_dict</h4><p>返回一个包含_parameters、_buffers、各个modules的statedict的OrderedDict()</p>
<h4 id="load-state-dict"><a href="#load-state-dict" class="headerlink" title="load_state_dict"></a>load_state_dict</h4><p>读入一个OrderDict对象</p>
<h3 id="torch-nn-Sequential"><a href="#torch-nn-Sequential" class="headerlink" title="torch.nn.Sequential"></a>torch.nn.Sequential</h3><p>一个序列容器，其中的modules将按照加入的顺序执行。该容器有两种初始化方式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">model = nn.Sequential(</div><div class="line">            nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</div><div class="line">            nn.ReLU(),</div><div class="line">            nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</div><div class="line">            nn.ReLU()</div><div class="line">    )</div><div class="line"><span class="comment"># or using OrderedDict</span></div><div class="line">model = nn.Sequential(</div><div class="line">    OrderedDict([</div><div class="line">        (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</div><div class="line">        (<span class="string">'relu1'</span>, nn.ReLU()),</div><div class="line">        (<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</div><div class="line">        (<span class="string">'relu2'</span>, nn.ReLU())</div><div class="line">    ])</div><div class="line">    )</div></pre></td></tr></table></figure></p>
<h3 id="torch-nn-ModuleList"><a href="#torch-nn-ModuleList" class="headerlink" title="torch.nn.ModuleList"></a>torch.nn.ModuleList</h3></div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://worldhellooo.github.io/2018/04/26/pytorch doc阅读笔记/" data-id="cjggojkv30000sgsebtbf91qe" class="article-share-link">分享到</a><div class="tags"></div><div class="post-nav"><a href="/2018/04/18/大数据竞赛经验谈/" class="next">大数据竞赛经验谈</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://worldhellooo.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Computer-Vision/">Computer Vision</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GUI/">GUI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learing/">Reinforcement Learing</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Visual-Tracking/">Visual Tracking</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/competition/">competition</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/笔记/">笔记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/增强学习/" style="font-size: 15px;">增强学习</a> <a href="/tags/增强学习-目标追踪/" style="font-size: 15px;">增强学习 目标追踪</a> <a href="/tags/实例搜索-孪生结果-目标追踪/" style="font-size: 15px;">实例搜索 孪生结果 目标追踪</a> <a href="/tags/目标追踪/" style="font-size: 15px;">目标追踪</a> <a href="/tags/RoIPooling-翻译/" style="font-size: 15px;">RoIPooling 翻译</a> <a href="/tags/目标追踪-研究背景-主流思路/" style="font-size: 15px;">目标追踪 研究背景 主流思路</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/26/pytorch doc阅读笔记/">pytorch doc阅读笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/18/大数据竞赛经验谈/">大数据竞赛经验谈</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/12/基于PyQt的多源图像文字描述生成软件界面/">基于PyQt的多源图像文字描述生成软件界面</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/翻译 Region of interest pooling explained/">翻译 Region of interest pooling explained</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/深度网络训练中的一些参数/">深度网络训练中的一些技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/目标追踪经典算法系列/">目标追踪算法之研究背景</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/SINT/">Introduction of Siamese Instance Search for Tracking</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/SiamFC改进思路/">SiamFC算法改进思路</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/hello-world/">Hello World</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/08/Background of Reinforcement Learning/">Background of Reinforcement Learning</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://blog.jcix.top" title="Zhangjc's blog" target="_blank">Zhangjc's blog</a><ul></ul><a href="http://chaway.github.io" title="Chaway's blog" target="_blank">Chaway's blog</a><ul></ul><a href="http://www.votchallenge.net" title="VOT Challenge" target="_blank">VOT Challenge</a><ul></ul><a href="http://www.visual-tracking.net" title="OTB Benchmark" target="_blank">OTB Benchmark</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">WorldHellooo's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>